{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:00:27.996473Z",
     "start_time": "2025-11-09T16:00:27.852079Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../gemma_annotation_original.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m median\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m annotated_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../gemma_annotation_original.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ------------------ helper functions ------------------\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_nfkc\u001b[39m(s: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../gemma_annotation_original.csv'"
     ]
    }
   ],
   "source": [
    "import json, re, unicodedata\n",
    "import pandas as pd\n",
    "from statistics import median\n",
    "from collections import Counter\n",
    "\n",
    "annotated_data = pd.read_csv(\"../gemma_annotation.csv\")\n",
    "\n",
    "# ------------------ helper functions ------------------\n",
    "def _nfkc(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def num(x):\n",
    "    \"\"\"Parse numbers robustly: NFKC, comma decimals, ASCII/Unicode fractions, '1½' & '1 ½', tolerate U+2044.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)): return None\n",
    "    s = _nfkc(str(x)).strip().lower()\n",
    "    s = s.replace(\",\", \".\").replace(\"⁄\", \"/\")\n",
    "    s = re.sub(r\"(\\d)([½¼¾⅓⅔⅛⅜⅝⅞])\", r\"\\1 \\2\", s)    # \"1½\" -> \"1 ½\"\n",
    "    s = re.split(r\"\\s+[a-zäöüß]+\", s, maxsplit=1)[0]  # keep leading numeric chunk\n",
    "    UF = {\"½\":0.5,\"¼\":0.25,\"¾\":0.75,\"⅓\":1/3,\"⅔\":2/3,\"⅛\":0.125,\"⅜\":0.375,\"⅝\":0.625,\"⅞\":0.875}\n",
    "    total, seen = 0.0, False\n",
    "    for tok in s.split():\n",
    "        if tok in UF:\n",
    "            total += UF[tok]; seen = True; continue\n",
    "        if re.fullmatch(r\"\\d+/\\d+\", tok):\n",
    "            n,d = tok.split(\"/\")\n",
    "            try: total += float(n)/float(d); seen = True; continue\n",
    "            except: break\n",
    "        try:\n",
    "            total += float(tok); seen = True; continue\n",
    "        except:\n",
    "            break\n",
    "    return total if seen else None\n",
    "\n",
    "def clean_unit(u):\n",
    "    if u is None or (isinstance(u, float) and pd.isna(u)): return \"\"\n",
    "    return _nfkc(str(u)).lower().strip().replace(\".\",\"\").replace(\"⁄\",\"/\")\n",
    "\n",
    "def parse_ann(raw):\n",
    "    if isinstance(raw, dict): return raw\n",
    "    if not isinstance(raw, str): return None\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return json.loads(raw.replace('\"\"','\"'))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def pick_amount(d):\n",
    "    for k in (\"gewicht\",\"volumen\",\"menge\",\"anzahl\"):\n",
    "        v = num(d.get(k))\n",
    "        if v is not None: return k, v\n",
    "    return None, None\n",
    "\n",
    "# ------------------ canonical targets ------------------\n",
    "CANON_G, CANON_L, CANON_P = \"g\", \"liter\", \"stück\"\n",
    "\n",
    "# Base map (extended later with tokens actually present)\n",
    "BASE = {\n",
    "    \"g\": (1.0, CANON_G), \"kg\": (1000.0, CANON_G), \"mg\": (0.001, CANON_G),\n",
    "    \"l\": (1.0, CANON_L), \"ml\": (0.001, CANON_L), \"cl\": (0.01, CANON_L),\n",
    "    \"stk\": (1.0, CANON_P), \"stück\": (1.0, CANON_P), \"stueck\": (1.0, CANON_P), \"piece\": (1.0, CANON_P)\n",
    "}\n",
    "\n",
    "# ------------------ mine unit-like tokens after numbers from `amount` ------------------\n",
    "VULGAR = \"½¼¾⅓⅔⅛⅜⅝⅞\"\n",
    "NUM = rf\"(?:\\d+[.,]?\\d*|\\d+/\\d+|[{VULGAR}])\"\n",
    "RANGE_SEP = r\"(?:\\s*(?:-|–|—|to|bis)\\s*)\"\n",
    "NUMRANGE = rf\"{NUM}(?:{RANGE_SEP}{NUM})?\"\n",
    "WORD = r\"[a-zA-ZäöüÄÖÜß]+\\.?\"\n",
    "SYMBOL = r\"[%%°]+\"\n",
    "pat_space = re.compile(rf\"{NUMRANGE}\\s*({WORD}|{SYMBOL})\", re.IGNORECASE)\n",
    "pat_glued = re.compile(rf\"({NUM})([a-zA-ZäöüÄÖÜß]+\\.?)\", re.IGNORECASE)\n",
    "\n",
    "def token_after_number(text: str):\n",
    "    if not isinstance(text, str) or not text.strip(): return None\n",
    "    t = _nfkc(text).strip().replace(\",\", \".\").replace(\"⁄\", \"/\")\n",
    "    t = re.sub(rf\"(\\d)([{VULGAR}])\", r\"\\1 \\2\", t)\n",
    "    m = pat_space.search(t)\n",
    "    if m: return m.group(1).lower().rstrip(\".\")\n",
    "    m2 = pat_glued.search(t)  # search catches \"ca. 200g\", \"≈200g\"\n",
    "    if m2: return m2.group(2).lower().rstrip(\".\")\n",
    "    return None\n",
    "\n",
    "present_tokens_amount = set()\n",
    "token_counter = Counter()\n",
    "for raw in annotated_data.get(\"amount\", pd.Series([], dtype=str)).astype(str):\n",
    "    tok = token_after_number(raw)\n",
    "    if tok:\n",
    "        present_tokens_amount.add(tok)\n",
    "        token_counter[tok] += 1\n",
    "\n",
    "# Also collect units that appear inside annotations\n",
    "present_units_ann = set()\n",
    "for raw in annotated_data.get(\"ingr_annotation\", pd.Series([], dtype=str)).astype(str):\n",
    "    ann = parse_ann(raw) if isinstance(raw, str) else None\n",
    "    if ann:\n",
    "        present_units_ann.add(clean_unit(ann.get(\"einheit\")))\n",
    "\n",
    "present_tokens_all = present_tokens_amount | present_units_ann\n",
    "\n",
    "# ------------------ alias tables (used ONLY if token is present) ------------------\n",
    "MASS_ALIAS = {\n",
    "    \"gram\":\"g\",\"grams\":\"g\",\"gramm\":\"g\",\"gramme\":\"g\",\"grammes\":\"g\",\"gr\":\"g\",\"g.\":\"g\"\n",
    "}\n",
    "VOLUME_ALIAS = {\n",
    "    \"liter\":\"l\",\"litre\":\"l\",\"liters\":\"l\",\"litres\":\"l\",\"ltr\":\"l\",\"lt\":\"l\",\"l.\":\"l\",\n",
    "    \"milliliter\":\"ml\",\"millilitre\":\"ml\",\"㎖\":\"ml\",\"mℓ\":\"ml\",\"mililiter\":\"ml\"\n",
    "}\n",
    "PIECE_ALIAS = {\n",
    "    \"st\":\"stk\",\"st.\":\"stk\",\"pcs\":\"stk\",\"pc\":\"stk\",\"pieces\":\"stk\",\"stuck\":\"stk\",\n",
    "    # additions seen\n",
    "    \"stck\":\"stk\",\"stücke\":\"stk\"\n",
    "}\n",
    "\n",
    "# Ambiguous *volume* units from our dataset → got it from tokens_present.csv\n",
    "AMB_ALIASES = {\n",
    "    \"el\": {\"el\",\"esslöffel\",\"essloeffel\"},\n",
    "    \"tl\": {\"tl\",\"teelöffel\",\"teeloeffel\"},\n",
    "    \"tasse\": {\"tasse\",\"tassen\"},\n",
    "    \"becher\": {\"becher\"},\n",
    "    \"glas\": {\"glas\",\"gläser\",\"glaeser\"},\n",
    "    \"prise\": {\"prise\"},\n",
    "    \"msp\": {\"msp\",\"messerspitze\"},\n",
    "    \"schuss\": {\"schuss\"},\n",
    "    \"spritzer\": {\"spritzer\", \"spr\"},\n",
    "    \"tropfen\": {\"tropfen\"},\n",
    "    \"bestecklöffel\": {\"bestecklöffel\"},\n",
    "}\n",
    "DEFAULT_ML = {\n",
    "    \"el\": 15, \"tl\": 5, \"tasse\": 250, \"becher\": 250, \"glas\": 200, \"prise\": 0.5, \"msp\": 0.5,\n",
    "    \"schuss\": 20,          # ~10–20 ml; set 20 ml\n",
    "    \"spritzer\": 2,         # small splash\n",
    "    \"tropfen\": 0.05,       # per drop\n",
    "    \"bestecklöffel\": 10,   # between TL (5) and EL (15)\n",
    "}\n",
    "\n",
    "# Packaging/count (non-liquid ambiguous) → pieces (only if present)\n",
    "PIECEY_ALIASES = {\n",
    "    \"päckchen\": {\"päckchen\",\"packchen\",\"päck.\",\"päck\",\"packung\",\"pkg\"},\n",
    "    \"dose\": {\"dose\",\"dosen\"},\n",
    "    \"flasche\": {\"flasche\",\"flaschen\"},\n",
    "    \"bund\": {\"bund\",\"bunde\"},\n",
    "    \"kopf\": {\"kopf\",\"köpfe\",\"koepfe\"},\n",
    "    \"scheibe\": {\"scheibe\",\"scheiben\"},\n",
    "    \"zehe\": {\"zehe\",\"zehen\"},\n",
    "    \"stange\": {\"stange\",\"stangen\"},\n",
    "    \"beutel\": {\"beutel\"},\n",
    "    \"tüte\": {\"tüte\",\"tuete\"},\n",
    "    # NEW from your tokens\n",
    "    \"blatt\": {\"blatt\",\"blätter\"},\n",
    "    \"bündel\": {\"bündel\",\"bd\"},\n",
    "    \"frucht\": {\"frucht\"},\n",
    "    \"hand\": {\"hand\"},\n",
    "    \"stiel\": {\"stiel\"},\n",
    "    \"würfel\": {\"würfel\"},\n",
    "    \"zweig\": {\"zweig\",\"zweige\"},\n",
    "    \"knolle\": {\"knolle\"},\n",
    "    \"pack\": {\"pack\"},\n",
    "    \"schüssel\": {\"schüssel\"},\n",
    "}\n",
    "\n",
    "def extend_base_from_present(base: dict, tokens: set):\n",
    "    for t, tgt in MASS_ALIAS.items():\n",
    "        if t in tokens: base[t] = base.get(tgt, (1.0, CANON_G))\n",
    "    for t, tgt in VOLUME_ALIAS.items():\n",
    "        if t in tokens: base[t] = base.get(tgt, (1.0, CANON_L) if tgt==\"l\" else (0.001, CANON_L))\n",
    "    for t, tgt in PIECE_ALIAS.items():\n",
    "        if t in tokens: base[t] = base.get(tgt, (1.0, CANON_P))\n",
    "    # packaging/count → pieces (only if present)\n",
    "    for canon, aliases in PIECEY_ALIASES.items():\n",
    "        if any(a in tokens for a in aliases) or canon in tokens:\n",
    "            for a in aliases | {canon}:\n",
    "                if a in tokens: base[a] = (1.0, CANON_P)\n",
    "\n",
    "extend_base_from_present(BASE, present_tokens_all)\n",
    "\n",
    "# ambiguous volume units we will use (present and defaults)\n",
    "def canon_amb(u_raw: str) -> str:\n",
    "    u = clean_unit(u_raw)\n",
    "    for c, aliases in AMB_ALIASES.items():\n",
    "        if u == c or u in aliases: return c\n",
    "    return u\n",
    "\n",
    "amb_from_amount = {c for c, aliases in AMB_ALIASES.items()\n",
    "                   if any(tok == c or tok in aliases for tok in present_tokens_amount)}\n",
    "amb_from_ann = {canon_amb(u) for u in present_units_ann}\n",
    "AMB_USED = sorted((amb_from_amount | amb_from_ann) & set(DEFAULT_ML.keys()))\n",
    "\n",
    "# summary (defaults only)\n",
    "def count_in_amount_for(canon: str) -> int:\n",
    "    aliases = AMB_ALIASES.get(canon, {canon})\n",
    "    return sum(token_counter.get(a, 0) for a in (aliases | {canon}))\n",
    "\n",
    "def count_in_ann_for(canon: str) -> int:\n",
    "    cnt = 0\n",
    "    for raw in annotated_data.get(\"ingr_annotation\", pd.Series([], dtype=str)).astype(str):\n",
    "        ann = parse_ann(raw)\n",
    "        if not ann: continue\n",
    "        if canon_amb(ann.get(\"einheit\")) == canon:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "summary_rows, ML_PER = [], {}\n",
    "for u in AMB_USED:\n",
    "    chosen = DEFAULT_ML[u]\n",
    "    ML_PER[u] = chosen\n",
    "    summary_rows.append({\n",
    "        \"unit\": u,\n",
    "        \"evidence_n\": 0,\n",
    "        \"chosen_ml_per_unit\": chosen,\n",
    "        \"source\": \"default_fallback\",\n",
    "        \"count_in_amount\": count_in_amount_for(u),\n",
    "        \"count_in_annotation\": count_in_ann_for(u)\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows).sort_values(\"unit\")\n",
    "df_summary.to_csv(\"ambiguous_summary.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Also dump the plain token counts we saw in `amount`\n",
    "pd.DataFrame(sorted(token_counter.items(), key=lambda x: (-x[1], x[0])),\n",
    "             columns=[\"token\",\"count\"]).to_csv(\"tokens_present.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# And a simple presence report for ambiguous units (even if not in AMB_USED)\n",
    "presence_rows = []\n",
    "for u, aliases in AMB_ALIASES.items():\n",
    "    presence_rows.append({\n",
    "        \"unit\": u,\n",
    "        \"present_in_amount\": int(any(a in present_tokens_amount for a in (aliases|{u}))),\n",
    "        \"present_in_annotation\": int(any(canon_amb(x)==u for x in present_units_ann)),\n",
    "        \"default_ml\": DEFAULT_ML[u]\n",
    "    })\n",
    "pd.DataFrame(presence_rows).sort_values(\"unit\").to_csv(\"ambiguous_presence.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------ build unit map (defaults for AMB_USED) ------------------\n",
    "unit_map = dict(BASE)\n",
    "for k, ml in ML_PER.items():  # liters per unit for ambiguous\n",
    "    unit_map[k] = (ml/1000.0, CANON_L)\n",
    "\n",
    "# ------------------ normalize rows ------------------\n",
    "vals, units = [], []\n",
    "for _, row in annotated_data.iterrows():\n",
    "    try:\n",
    "        ann = parse_ann(row.get(\"ingr_annotation\"))\n",
    "        if not ann: raise ValueError(\"no_json\")\n",
    "        key, amt = pick_amount(ann)\n",
    "        if amt is None:\n",
    "            amt = num(row.get(\"amount\"))\n",
    "            if amt is None: amt = 1.0\n",
    "        u = canon_amb(clean_unit(ann.get(\"einheit\")))\n",
    "        if u in unit_map:\n",
    "            f, target = unit_map[u]\n",
    "            val, unit = float(amt)*f, target\n",
    "        else:\n",
    "            if key in (\"menge\",\"anzahl\"):   val, unit = float(amt), CANON_P\n",
    "            elif key == \"gewicht\":          val, unit = float(amt), CANON_G\n",
    "            elif key == \"volumen\":          val, unit = float(amt), CANON_L\n",
    "            else:                           val, unit = float(amt), CANON_P\n",
    "    except Exception:\n",
    "        v = num(row.get(\"amount\"))\n",
    "        val, unit = (v if v is not None else 1.0), CANON_P\n",
    "    vals.append(val); units.append(unit)\n",
    "\n",
    "annotated_data[\"norm_value\"] = vals\n",
    "annotated_data[\"norm_unit\"]  = units\n",
    "\n",
    "# ------------------ duplicates diagnostics ------------------\n",
    "dup_subset = [c for c in [\"ingredient\",\"amount\",\"ingr_annotation\"] if c in annotated_data.columns]\n",
    "if dup_subset:\n",
    "    dup_rows = annotated_data[annotated_data.duplicated(subset=dup_subset, keep=False)].copy()\n",
    "    dup_rows = dup_rows.sort_values(dup_subset)\n",
    "    dup_rows.to_csv(\"duplicates.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    dup_groups = (annotated_data\n",
    "                  .groupby(dup_subset, dropna=False)\n",
    "                  .size().reset_index(name=\"count\")\n",
    "                  .query(\"count > 1\")\n",
    "                  .sort_values(\"count\", ascending=False))\n",
    "    dup_groups.to_csv(\"duplicates_groups.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------ ingredient-based profiling (soft duplicates) ------------------\n",
    "def get_ing_key(row):\n",
    "    # Prefer annotated ingredient name if present; else fall back to raw text\n",
    "    ann_amt = parse_ann(row.get(\"amount_annotation\"))\n",
    "    name = (ann_amt or {}).get(\"zutat\")\n",
    "    if not name:\n",
    "        ann_ing = parse_ann(row.get(\"ingr_annotation\"))\n",
    "        name = (ann_ing or {}).get(\"zutat\") or row.get(\"ingredient\", \"\")\n",
    "    return _nfkc(str(name)).lower().strip()\n",
    "\n",
    "def get_ann_key(row):\n",
    "    ann = parse_ann(row.get(\"ingr_annotation\"))\n",
    "    if not ann: return None\n",
    "    for k in (\"gewicht\",\"volumen\",\"menge\",\"anzahl\"):\n",
    "        if ann.get(k) is not None:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def get_ann_unit(row):\n",
    "    ann = parse_ann(row.get(\"ingr_annotation\"))\n",
    "    return clean_unit(ann.get(\"einheit\")) if ann else \"\"\n",
    "\n",
    "df = annotated_data.copy()\n",
    "df[\"ing_key\"]   = df.apply(get_ing_key, axis=1)\n",
    "df[\"ann_key\"]   = df.apply(get_ann_key, axis=1)      # which JSON field carried the amount\n",
    "df[\"ann_unit\"]  = df.apply(get_ann_unit, axis=1)     # raw unit from JSON (cleaned)\n",
    "\n",
    "def counter_to_sorted_dict(c: Counter):\n",
    "    return dict(sorted(c.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "\n",
    "profiles = []\n",
    "for ing, grp in df.groupby(\"ing_key\", dropna=False):\n",
    "    profiles.append({\n",
    "        \"ingredient\": ing,\n",
    "        \"n_rows\": len(grp),\n",
    "        \"n_amount_unique\": int(grp[\"amount\"].nunique() if \"amount\" in grp else 0),\n",
    "        \"ann_key_counts\": counter_to_sorted_dict(Counter(grp[\"ann_key\"].fillna(\"None\"))),\n",
    "        \"ann_unit_counts\": counter_to_sorted_dict(Counter([u for u in grp[\"ann_unit\"] if u])),\n",
    "        \"norm_unit_counts\": counter_to_sorted_dict(Counter(grp[\"norm_unit\"])),\n",
    "    })\n",
    "\n",
    "profiles_df = pd.DataFrame(profiles).sort_values([\"ingredient\",\"n_rows\"], ascending=[True, False])\n",
    "profiles_df.to_csv(\"ingredient_profiles.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "conflicts = []\n",
    "for ing, grp in df.groupby(\"ing_key\", dropna=False):\n",
    "    nu = Counter(grp[\"norm_unit\"])\n",
    "    if len(nu) > 1:\n",
    "        total = sum(nu.values())\n",
    "        top_unit, top_cnt = max(nu.items(), key=lambda kv: kv[1])\n",
    "        conflicts.append({\n",
    "            \"ingredient\": ing,\n",
    "            \"norm_unit_counts\": counter_to_sorted_dict(nu),\n",
    "            \"majority_unit\": top_unit,\n",
    "            \"majority_share\": round(top_cnt / total, 3),\n",
    "            \"n_rows\": total\n",
    "        })\n",
    "conflicts_df = pd.DataFrame(conflicts).sort_values([\"majority_share\",\"n_rows\"], ascending=[True, False])\n",
    "conflicts_df.to_csv(\"ingredient_conflicts.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "consensus = []\n",
    "for ing, grp in df.groupby(\"ing_key\", dropna=False):\n",
    "    nu = Counter(grp[\"norm_unit\"])\n",
    "    total = sum(nu.values())\n",
    "    top_unit, top_cnt = max(nu.items(), key=lambda kv: kv[1])\n",
    "    consensus.append({\n",
    "        \"ingredient\": ing,\n",
    "        \"suggested_unit\": top_unit,\n",
    "        \"support\": top_cnt,\n",
    "        \"support_share\": round(top_cnt / total, 3),\n",
    "        \"n_rows\": total\n",
    "    })\n",
    "consensus_df = pd.DataFrame(consensus).sort_values([\"support_share\",\"n_rows\"], ascending=[False, False])\n",
    "consensus_df.to_csv(\"unit_consensus.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "#  preview & save\n",
    "print(\"Present tokens from amount:\", sorted(present_tokens_amount))\n",
    "print(\"Present units from annotation:\", sorted(present_units_ann))\n",
    "print(f\"BASE extended with present aliases. Keys: {len(BASE)}\")\n",
    "print(f\"Ambiguous VOLUME units considered (present ∩ defaults): {AMB_USED}\")\n",
    "print(\"Wrote: ambiguous_summary.csv, ambiguous_presence.csv, tokens_present.csv\")\n",
    "if dup_subset:\n",
    "    print(\"Wrote: duplicates.csv, duplicates_groups.csv\")\n",
    "print(\"Wrote: ingredient_profiles.csv, ingredient_conflicts.csv, unit_consensus.csv\")\n",
    "\n",
    "try:\n",
    "    display(df_summary.head(12))\n",
    "    display(annotated_data[[\"ingredient\",\"amount\",\"ingr_annotation\",\"norm_value\",\"norm_unit\"]].head(60))\n",
    "    display(consensus_df.head(20))\n",
    "    display(conflicts_df.head(20))\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "NORMALIZED_CSV = \"normalized_units.csv\"\n",
    "annotated_data.to_csv(NORMALIZED_CSV, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b2251bb46caee46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:01:33.732173Z",
     "start_time": "2025-11-09T05:01:33.730327Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c29a533a9d464ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:01:33.735313Z",
     "start_time": "2025-11-09T05:01:33.732173Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
